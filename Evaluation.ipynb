{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. most popular 1000 Python Github repo\n",
    "2. filter top N frequently asked questions on StackOverflow(17000->2000) (question viewed+votes)\n",
    "3. Verify if the StackOverflow code snippet exist in 1000 repo\n",
    "    - ElasticSearch\n",
    "    - manually choose 100 questions from ElasticSearch result\n",
    "4. use StackOverflow questions as input of the model, and manually evalute if the top 10 results has correct ansers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Evaluation 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replaced the 4th step of the earlier evaluation methods with:<br>\n",
    "- first taking the top 10 results retrieved by NCS, and for each retrieved method, getting a similarity score between the groundâ€“truth code snippet and the method. \n",
    "- choose a threshold that minimize false positive\n",
    "=>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path to target files\n",
    "st=time()\n",
    "path_wordembedding=\"data/embeddings.txt\"\n",
    "path_docembedding=\"data/document_embeddings.csv\"\n",
    "path_stackoverflow=\"data/stack_overflow_data_all.csv\"\n",
    "\n",
    "# change hyperparameters\n",
    "vocab_size=200\n",
    "window_size=5\n",
    "\n",
    "#StackOverflow start id\n",
    "start_idx=0\n",
    "end_idx=10 #will actually run to end_idx-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load StackOverflow data\n",
    "st=time()\n",
    "df_stack_overflow=pd.read_csv(path_stackoverflow)\n",
    "print(\"Dimension of StackOverflow data: {}\".format(df_stack_overflow.shape))\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wordembedding: representation of words\n",
    "st=time()\n",
    "trained_ft_vectors = KeyedVectors.load_word2vec_format(path_wordembedding)\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load document embedding: representation of each source code function\n",
    "st=time()\n",
    "document_embeddings=np.loadtxt(fname=path_docembedding, delimiter=\",\")\n",
    "print(\"Dimension of the document embedding: {}\".format(document_embeddings.shape))\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize a word represenatation vector that its L2 norm is 1.\n",
    "# we do this so that the cosine similarity reduces to a simple dot product\n",
    "\n",
    "def normalize(word_representations):\n",
    "    for word in word_representations:\n",
    "        total=0\n",
    "        for key in word_representations[word]:\n",
    "            total+=word_representations[word][key]*word_representations[word][key]\n",
    "            \n",
    "        total=math.sqrt(total)\n",
    "        for key in word_representations[word]:\n",
    "            word_representations[word][key]/=total\n",
    "\n",
    "def dictionary_dot_product(dict1, dict2):\n",
    "    dot=0\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            dot+=dict1[key]*dict2[key]\n",
    "    return dot\n",
    "\n",
    "def find_sim(word_representations, query):\n",
    "    if query not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query)\n",
    "        return None\n",
    "    \n",
    "    scores={}\n",
    "    for word in word_representations:\n",
    "        cosine=dictionary_dot_product(word_representations[query], word_representations[word])\n",
    "        scores[word]=cosine\n",
    "    return scores\n",
    "\n",
    "# Find the K words with highest cosine similarity to a query in a set of word_representations\n",
    "def find_nearest_neighbors(word_representations, query, K):\n",
    "    scores=find_sim(word_representations, query)\n",
    "    if scores != None:\n",
    "        sorted_x = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "            print(\"%s\\t%s\\t%.5f\" % (idx,k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_document(question, word_embedding, doc_embedding, num=10):\n",
    "    \"\"\"Return the functions that are most relevant to the natual language question.\n",
    "\n",
    "    Args:\n",
    "        question: A string. A Question from StackOverflow. \n",
    "        word_embedding: Word embedding generated from codebase.\n",
    "        doc_embedding: Document embedding generated from codebase\n",
    "        num: The number of top similar functions to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of indices of the top NUM related functions to the QUESTION in the WORD_EMBEDDING.\n",
    "    \n",
    "    \"\"\"\n",
    "    # convert QUESTION to a vector\n",
    "    tokenized_ques=question.split()\n",
    "    vec_ques=np.zeros((1,document_embeddings.shape[1])) #vocab_size\n",
    "    token_count=0\n",
    "    has_token_in_embedding=False\n",
    "    for token in tokenized_ques:\n",
    "        if token in word_embedding:\n",
    "            has_token_in_embedding=True\n",
    "            vec_ques+=word_embedding[token]\n",
    "            token_count+=1\n",
    "    \n",
    "    if has_token_in_embedding:\n",
    "        mean_vec_ques=vec_ques/token_count\n",
    "    \n",
    "    \n",
    "        # compute similarity between this question and each of the source code snippets\n",
    "        cosine_sim=[]\n",
    "        for idx, doc in enumerate(document_embeddings):\n",
    "            #[TODO] fix dimension\n",
    "\n",
    "            try:\n",
    "                cosine_sim.append(cosine_similarity(mean_vec_ques, doc.reshape(1, -1))[0][0])\n",
    "            except ValueError:\n",
    "                print(question)\n",
    "                print(vec_ques, token_count)\n",
    "                print(mean_vec_ques)\n",
    "                print(doc.reshape(1, -1))\n",
    "        # get top `num` similar functions\n",
    "        result_func_id=np.array(cosine_sim).argsort()[-num:][::-1]\n",
    "        result_similarity=np.sort(np.array(cosine_sim))[-num:][::-1]\n",
    "    else:\n",
    "        result_func_id=np.nan\n",
    "        result_similarity=np.nan\n",
    "    return result_func_id, result_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit number of questions\n",
    "df_stack_overflow_partial=df_stack_overflow.iloc[start_idx:end_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=time()\n",
    "list_most_relevant_doc=[]\n",
    "list_most_relevant_sim=[]\n",
    "for idx in range(len(df_stack_overflow_partial)): \n",
    "    question=df_stack_overflow_partial.iloc[idx][\"Question Title\"]\n",
    "    \n",
    "    most_relevant_doc, most_relevant_sim=get_most_relevant_document(question, trained_ft_vectors, document_embeddings)\n",
    "    list_most_relevant_doc.append(most_relevant_doc)\n",
    "    list_most_relevant_sim.append(most_relevant_sim)\n",
    "df_stack_overflow_partial[\"func_id\"]=list_most_relevant_doc\n",
    "df_stack_overflow_partial[\"sim\"]=list_most_relevant_sim\n",
    "print(\"Run time: {} s\".format(time()-st)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result\n",
    "df_stack_overflow_partial.to_pickle(\"data/SO_similarity_{}_{}.pkl\".format(start_idx, end_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stack_overflow_partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stack_overflow_partial=pd.read_pickle(\"data/SO_similarity_0_10.pkl\")\n",
    "df_stack_overflow_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stack_overflow_partial[df_stack_overflow_partial[\"Post Link\"]==48211001][\"Question Title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stack_overflow_partial[df_stack_overflow_partial[\"Post Link\"]==48211001][\"func_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_py100k=pd.read_pickle(\"data/py100k.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_py100k[700000:700100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_id=[]\n",
    "for i in [260771, 275794, 428754, 372502, 360950, 284871, 412289, 412286, 11140, 412288]:\n",
    "    #list_data_id.append(df_py100k.iloc[i][\"data_id\"])\n",
    "    print(df_py100k.iloc[i])\n",
    "    print()\n",
    "#df_py100k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
