{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. most popular 1000 Python Github repo\n",
    "2. filter top N frequently asked questions on StackOverflow(17000->2000) (question viewed+votes)\n",
    "3. Verify if the StackOverflow code snippet exist in 1000 repo\n",
    "    - ElasticSearch\n",
    "    - manually choose 100 questions from ElasticSearch result\n",
    "4. use StackOverflow questions as input of the model, and manually evalute if the top 10 results has correct ansers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Evaluation 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replaced the 4th step of the earlier evaluation methods with:<br>\n",
    "- first taking the top 10 results retrieved by NCS, and for each retrieved method, getting a similarity score between the groundâ€“truth code snippet and the method. \n",
    "- choose a threshold that minimize false positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path to target files\n",
    "st=time()\n",
    "path_wordembedding=\"data/embeddings.txt\"\n",
    "path_docembedding=\"data/document_embeddings.csv\"\n",
    "path_stackoverflow=\"data/stack_overflow_data_all.csv\"\n",
    "\n",
    "# change hyperparameters\n",
    "vocab_size=200\n",
    "window_size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of StackOverflow data: (30510, 7)\n",
      "Run time: 1.1801400184631348 s\n"
     ]
    }
   ],
   "source": [
    "# load StackOverflow data\n",
    "st=time()\n",
    "df_stack_overflow=pd.read_csv(path_stackoverflow)\n",
    "print(\"Dimension of StackOverflow data: {}\".format(df_stack_overflow.shape))\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 0.41855907440185547 s\n"
     ]
    }
   ],
   "source": [
    "# load wordembedding: representation of words\n",
    "st=time()\n",
    "trained_ft_vectors = KeyedVectors.load_word2vec_format(path_wordembedding)\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the document embedding: (1038, 200)\n",
      "Run time: 0.21023273468017578 s\n"
     ]
    }
   ],
   "source": [
    "# load document embedding: representation of each source code function\n",
    "st=time()\n",
    "document_embeddings=np.loadtxt(fname=path_docembedding, delimiter=\",\")\n",
    "print(\"Dimension of the document embedding: {}\".format(document_embeddings.shape))\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize a word represenatation vector that its L2 norm is 1.\n",
    "# we do this so that the cosine similarity reduces to a simple dot product\n",
    "\n",
    "def normalize(word_representations):\n",
    "    for word in word_representations:\n",
    "        total=0\n",
    "        for key in word_representations[word]:\n",
    "            total+=word_representations[word][key]*word_representations[word][key]\n",
    "            \n",
    "        total=math.sqrt(total)\n",
    "        for key in word_representations[word]:\n",
    "            word_representations[word][key]/=total\n",
    "\n",
    "def dictionary_dot_product(dict1, dict2):\n",
    "    dot=0\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            dot+=dict1[key]*dict2[key]\n",
    "    return dot\n",
    "\n",
    "def find_sim(word_representations, query):\n",
    "    if query not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query)\n",
    "        return None\n",
    "    \n",
    "    scores={}\n",
    "    for word in word_representations:\n",
    "        cosine=dictionary_dot_product(word_representations[query], word_representations[word])\n",
    "        scores[word]=cosine\n",
    "    return scores\n",
    "\n",
    "# Find the K words with highest cosine similarity to a query in a set of word_representations\n",
    "def find_nearest_neighbors(word_representations, query, K):\n",
    "    scores=find_sim(word_representations, query)\n",
    "    if scores != None:\n",
    "        sorted_x = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "            print(\"%s\\t%s\\t%.5f\" % (idx,k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_document(question, word_embedding, doc_embedding, num=10):\n",
    "    \"\"\"Return the functions that are most relevant to the natual language question.\n",
    "\n",
    "    Args:\n",
    "        question: A string. A Question from StackOverflow. \n",
    "        word_embedding: Word embedding generated from codebase.\n",
    "        doc_embedding: Document embedding generated from codebase\n",
    "        num: The number of top similar functions to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of indices of the top NUM related functions to the QUESTION in the WORD_EMBEDDING.\n",
    "    \n",
    "    \"\"\"\n",
    "    # convert QUESTION to a vector\n",
    "    tokenized_ques=question.split()\n",
    "    vec_ques=np.zeros((1,document_embeddings.shape[1])) #vocab_size\n",
    "    token_count=0\n",
    "    has_token_in_embedding=False\n",
    "    for token in tokenized_ques:\n",
    "        if token in word_embedding:\n",
    "            has_token_in_embedding=True\n",
    "            vec_ques+=word_embedding[token]\n",
    "            token_count+=1\n",
    "    \n",
    "    if has_token_in_embedding:\n",
    "        mean_vec_ques=vec_ques/token_count\n",
    "    \n",
    "    \n",
    "        # compute similarity between this question and each of the source code snippets\n",
    "        cosine_sim=[]\n",
    "        for idx, doc in enumerate(document_embeddings):\n",
    "            #[TODO] fix dimension\n",
    "\n",
    "            try:\n",
    "                cosine_sim.append(cosine_similarity(mean_vec_ques, doc.reshape(1, -1))[0][0])\n",
    "            except ValueError:\n",
    "                print(question)\n",
    "                print(vec_ques, token_count)\n",
    "                print(mean_vec_ques)\n",
    "                print(doc.reshape(1, -1))\n",
    "        # get top `num` similar functions\n",
    "        result=np.array(cosine_sim).argsort()[-num:][::-1]\n",
    "    else:\n",
    "        result=np.nan\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 1.622117042541504 s\n"
     ]
    }
   ],
   "source": [
    "st=time()\n",
    "list_most_relevant_doc=[]\n",
    "for idx in range(len(df_stack_overflow)):\n",
    "    question=df_stack_overflow.iloc[idx][\"Question Title\"]\n",
    "    \n",
    "    most_relevant_doc=get_most_relevant_document(question, trained_ft_vectors, document_embeddings)\n",
    "    list_most_relevant_doc.append(most_relevant_doc)\n",
    "df_stack_overflow[\"func_id\"]=list_most_relevant_doc\n",
    "print(\"Run time: {} s\".format(time()-st)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post Link</th>\n",
       "      <th>Question Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Question Title</th>\n",
       "      <th>Question Content</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Tags</th>\n",
       "      <th>func_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48174398</td>\n",
       "      <td>4</td>\n",
       "      <td>435</td>\n",
       "      <td>New Dataframe column as a generic function of ...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;What is the fastest (and most effic...</td>\n",
       "      <td>&lt;p&gt;Let's try to analyze the problem for a seco...</td>\n",
       "      <td>&lt;python&gt;&lt;pandas&gt;&lt;dataframe&gt;&lt;vectorization&gt;</td>\n",
       "      <td>[448, 371, 504, 317, 403, 554, 409, 454, 452, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48211001</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>Python: How to update multiple address lines</td>\n",
       "      <td>&lt;p&gt;I'm currently working on a banking system f...</td>\n",
       "      <td>&lt;p&gt;Your issue is arguably in your &lt;code&gt;update...</td>\n",
       "      <td>&lt;python&gt;</td>\n",
       "      <td>[433, 490, 230, 149, 722, 527, 516, 973, 515, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48213278</td>\n",
       "      <td>1</td>\n",
       "      <td>2389</td>\n",
       "      <td>Implementing Otsu binarization from scratch py...</td>\n",
       "      <td>&lt;p&gt;It seems my implementation is incorrect and...</td>\n",
       "      <td>&lt;p&gt;I dont know if my implementation is alright...</td>\n",
       "      <td>&lt;python&gt;&lt;python-3.x&gt;&lt;image-processing&gt;&lt;compute...</td>\n",
       "      <td>[1011, 1003, 991, 275, 992, 994, 993, 990, 989...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48217471</td>\n",
       "      <td>2</td>\n",
       "      <td>311</td>\n",
       "      <td>Is it possible to check for anagram without us...</td>\n",
       "      <td>&lt;p&gt;I'm currently studying and I was told not t...</td>\n",
       "      <td>&lt;p&gt;The pythonic way of answering this question...</td>\n",
       "      <td>&lt;python&gt;&lt;algorithm&gt;</td>\n",
       "      <td>[433, 173, 172, 182, 184, 419, 183, 303, 350, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48217583</td>\n",
       "      <td>1</td>\n",
       "      <td>531</td>\n",
       "      <td>Accessing attributes of user defined object le...</td>\n",
       "      <td>&lt;p&gt;I have a class called LineString that consi...</td>\n",
       "      <td>&lt;p&gt;The cause for your error is the fact that y...</td>\n",
       "      <td>&lt;python&gt;&lt;class&gt;&lt;typeerror&gt;</td>\n",
       "      <td>[289, 108, 117, 175, 290, 287, 288, 427, 530, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Post Link  Question Score  ViewCount  \\\n",
       "0   48174398               4        435   \n",
       "1   48211001               0         56   \n",
       "2   48213278               1       2389   \n",
       "3   48217471               2        311   \n",
       "4   48217583               1        531   \n",
       "\n",
       "                                      Question Title  \\\n",
       "0  New Dataframe column as a generic function of ...   \n",
       "1       Python: How to update multiple address lines   \n",
       "2  Implementing Otsu binarization from scratch py...   \n",
       "3  Is it possible to check for anagram without us...   \n",
       "4  Accessing attributes of user defined object le...   \n",
       "\n",
       "                                    Question Content  \\\n",
       "0  <p><strong>What is the fastest (and most effic...   \n",
       "1  <p>I'm currently working on a banking system f...   \n",
       "2  <p>It seems my implementation is incorrect and...   \n",
       "3  <p>I'm currently studying and I was told not t...   \n",
       "4  <p>I have a class called LineString that consi...   \n",
       "\n",
       "                                              Answer  \\\n",
       "0  <p>Let's try to analyze the problem for a seco...   \n",
       "1  <p>Your issue is arguably in your <code>update...   \n",
       "2  <p>I dont know if my implementation is alright...   \n",
       "3  <p>The pythonic way of answering this question...   \n",
       "4  <p>The cause for your error is the fact that y...   \n",
       "\n",
       "                                                Tags  \\\n",
       "0         <python><pandas><dataframe><vectorization>   \n",
       "1                                           <python>   \n",
       "2  <python><python-3.x><image-processing><compute...   \n",
       "3                                <python><algorithm>   \n",
       "4                         <python><class><typeerror>   \n",
       "\n",
       "                                             func_id  \n",
       "0  [448, 371, 504, 317, 403, 554, 409, 454, 452, ...  \n",
       "1  [433, 490, 230, 149, 722, 527, 516, 973, 515, ...  \n",
       "2  [1011, 1003, 991, 275, 992, 994, 993, 990, 989...  \n",
       "3  [433, 173, 172, 182, 184, 419, 183, 303, 350, ...  \n",
       "4  [289, 108, 117, 175, 290, 287, 288, 427, 530, ...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save result\n",
    "df_stack_overflow.to_pickle(\"data/SO_similarity.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
