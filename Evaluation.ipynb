{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. most popular 1000 Python Github repo\n",
    "2. filter top N frequently asked questions on StackOverflow(17000->2000) (question viewed+votes)\n",
    "3. Verify if the StackOverflow code snippet exist in 1000 repo\n",
    "    - ElasticSearch\n",
    "    - manually choose 100 questions from ElasticSearch result\n",
    "4. use StackOverflow questions as input of the model, and manually evalute if the top 10 results has correct ansers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Evaluation 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replaced the 4th step of the earlier evaluation methods with:<br>\n",
    "- first taking the top 10 results retrieved by NCS, and for each retrieved method, getting a similarity score between the groundâ€“truth code snippet and the method. \n",
    "- choose a threshold that minimize false positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path to target files\n",
    "st=time()\n",
    "path_wordembedding=\"data/embeddings.txt\"\n",
    "path_docembedding=\"data/document_embeddings.csv\"\n",
    "path_stackoverflow=\"data/stack_overflow_data_all.csv\"\n",
    "\n",
    "# change hyperparameters\n",
    "vocab_size=200\n",
    "window_size=5\n",
    "\n",
    "#StackOverflow start id\n",
    "start_idx=0\n",
    "end_idx=10 #will actually run to end_idx-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of StackOverflow data: (30510, 7)\n",
      "Run time: 1.2091279029846191 s\n"
     ]
    }
   ],
   "source": [
    "# load StackOverflow data\n",
    "st=time()\n",
    "df_stack_overflow=pd.read_csv(path_stackoverflow)\n",
    "print(\"Dimension of StackOverflow data: {}\".format(df_stack_overflow.shape))\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 0.509882926940918 s\n"
     ]
    }
   ],
   "source": [
    "# load wordembedding: representation of words\n",
    "st=time()\n",
    "trained_ft_vectors = KeyedVectors.load_word2vec_format(path_wordembedding)\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the document embedding: (1038, 200)\n",
      "Run time: 0.2133772373199463 s\n"
     ]
    }
   ],
   "source": [
    "# load document embedding: representation of each source code function\n",
    "st=time()\n",
    "document_embeddings=np.loadtxt(fname=path_docembedding, delimiter=\",\")\n",
    "print(\"Dimension of the document embedding: {}\".format(document_embeddings.shape))\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize a word represenatation vector that its L2 norm is 1.\n",
    "# we do this so that the cosine similarity reduces to a simple dot product\n",
    "\n",
    "def normalize(word_representations):\n",
    "    for word in word_representations:\n",
    "        total=0\n",
    "        for key in word_representations[word]:\n",
    "            total+=word_representations[word][key]*word_representations[word][key]\n",
    "            \n",
    "        total=math.sqrt(total)\n",
    "        for key in word_representations[word]:\n",
    "            word_representations[word][key]/=total\n",
    "\n",
    "def dictionary_dot_product(dict1, dict2):\n",
    "    dot=0\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            dot+=dict1[key]*dict2[key]\n",
    "    return dot\n",
    "\n",
    "def find_sim(word_representations, query):\n",
    "    if query not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query)\n",
    "        return None\n",
    "    \n",
    "    scores={}\n",
    "    for word in word_representations:\n",
    "        cosine=dictionary_dot_product(word_representations[query], word_representations[word])\n",
    "        scores[word]=cosine\n",
    "    return scores\n",
    "\n",
    "# Find the K words with highest cosine similarity to a query in a set of word_representations\n",
    "def find_nearest_neighbors(word_representations, query, K):\n",
    "    scores=find_sim(word_representations, query)\n",
    "    if scores != None:\n",
    "        sorted_x = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "            print(\"%s\\t%s\\t%.5f\" % (idx,k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_document(question, word_embedding, doc_embedding, num=10):\n",
    "    \"\"\"Return the functions that are most relevant to the natual language question.\n",
    "\n",
    "    Args:\n",
    "        question: A string. A Question from StackOverflow. \n",
    "        word_embedding: Word embedding generated from codebase.\n",
    "        doc_embedding: Document embedding generated from codebase\n",
    "        num: The number of top similar functions to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of indices of the top NUM related functions to the QUESTION in the WORD_EMBEDDING.\n",
    "    \n",
    "    \"\"\"\n",
    "    # convert QUESTION to a vector\n",
    "    tokenized_ques=question.split()\n",
    "    vec_ques=np.zeros((1,document_embeddings.shape[1])) #vocab_size\n",
    "    token_count=0\n",
    "    has_token_in_embedding=False\n",
    "    for token in tokenized_ques:\n",
    "        if token in word_embedding:\n",
    "            has_token_in_embedding=True\n",
    "            vec_ques+=word_embedding[token]\n",
    "            token_count+=1\n",
    "    \n",
    "    if has_token_in_embedding:\n",
    "        mean_vec_ques=vec_ques/token_count\n",
    "    \n",
    "    \n",
    "        # compute similarity between this question and each of the source code snippets\n",
    "        cosine_sim=[]\n",
    "        for idx, doc in enumerate(document_embeddings):\n",
    "            #[TODO] fix dimension\n",
    "\n",
    "            try:\n",
    "                cosine_sim.append(cosine_similarity(mean_vec_ques, doc.reshape(1, -1))[0][0])\n",
    "            except ValueError:\n",
    "                print(question)\n",
    "                print(vec_ques, token_count)\n",
    "                print(mean_vec_ques)\n",
    "                print(doc.reshape(1, -1))\n",
    "        # get top `num` similar functions\n",
    "        result=np.array(cosine_sim).argsort()[-num:][::-1]\n",
    "    else:\n",
    "        result=np.nan\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit number of questions\n",
    "df_stack_overflow_partial=df_stack_overflow.iloc[start_idx:end_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-243c5b2ed2f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmost_relevant_doc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_most_relevant_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_ft_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlist_most_relevant_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmost_relevant_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_stack_overflow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"func_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist_most_relevant_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Run time: {} s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/capstone/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3368\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3369\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3370\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3372\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/capstone/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3444\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3445\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3446\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/capstone/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3629\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3630\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3631\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3632\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/.virtualenvs/capstone/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "st=time()\n",
    "list_most_relevant_doc=[]\n",
    "for idx in range(len(df_stack_overflow_partial)): \n",
    "    question=df_stack_overflow_partial.iloc[idx][\"Question Title\"]\n",
    "    \n",
    "    most_relevant_doc=get_most_relevant_document(question, trained_ft_vectors, document_embeddings)\n",
    "    list_most_relevant_doc.append(most_relevant_doc)\n",
    "df_stack_overflow_partial[\"func_id\"]=list_most_relevant_doc\n",
    "print(\"Run time: {} s\".format(time()-st)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result\n",
    "df_stack_overflow.to_pickle(\"data/SO_similarity.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
