{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[150k Python Dataset](\"https://eth-sri.github.io/py150\") from SRILAB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuous vector embedding of each code fragment at method–level granularity as \"document\".[13] FastText, a variation of Word2Vec algorithm.\n",
    "- Extracting Information from Source Code\n",
    "    - simple tokenizer: extract all words from source code by removing non–alphanumeric tokens.=> indifferenciable\n",
    "    - parser-based approach: traverse through the parse tree for each method, and extract information from the following syntactic categories. (Java-like)\n",
    "        - method name\n",
    "        - method invocation\n",
    "        - Enums\n",
    "        - String literals\n",
    "        - comments\n",
    "        - <strike>variable name</strike>\n",
    "- Building Vector Representations\n",
    "    - <strike>simply average embeddings</strike>\n",
    "    - Weighted average of all unique words in a document=> normalized tf-idf\n",
    "- Retrieval\n",
    "    - average the vector representations of constituent words to create a document embedding for the query sentence\n",
    "    - a standard similarity search algorithm to find the document vectors with closest cosine distance. => FAISS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: natural language queries <br>\n",
    "Output: related code fragments retrieved directly from Github code corpus<br><br>\n",
    "\n",
    "Map the Input into the same vector space as the codebase, and then calculate the vector distance of them in order to get the relevant result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric and choosing parameters of the model\n",
    "\n",
    "- Metric: select subsets of words from the document as simulated queries and then see if it can retrive the document, and then evaluate by the percentage of the documents that are retrieve back at top1 and top10. \n",
    "    - random benchmark test\n",
    "    - TF-IDF benchmark test => better performance\n",
    "- Parameters:\n",
    "    - embedding dimention=> 500\n",
    "    - three ways of combining word embeddings to document embeddings=> the conclusion is tf-idf better\n",
    "    - vector representation=> BM25 is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Functions: 1038\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>function_name</th>\n",
       "      <th>docstring</th>\n",
       "      <th>func_call</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>__getattribute__</td>\n",
       "      <td></td>\n",
       "      <td>__getattribute__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>__setattr__</td>\n",
       "      <td></td>\n",
       "      <td>ref,__setattr__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>main</td>\n",
       "      <td></td>\n",
       "      <td>setup,closing,ZmqProxy,consume_in_thread,wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test_vpnservice_create</td>\n",
       "      <td></td>\n",
       "      <td>create_stubs,first,AndReturn,ReplayAll,vpnserv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>test_vpnservices_get</td>\n",
       "      <td></td>\n",
       "      <td>create_stubs,AndReturn,ReplayAll,vpnservices_g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_id           function_name docstring  \\\n",
       "0       1        __getattribute__             \n",
       "1       1             __setattr__             \n",
       "2       2                    main             \n",
       "3       3  test_vpnservice_create             \n",
       "4       3    test_vpnservices_get             \n",
       "\n",
       "                                           func_call  \n",
       "0                                   __getattribute__  \n",
       "1                                    ref,__setattr__  \n",
       "2      setup,closing,ZmqProxy,consume_in_thread,wait  \n",
       "3  create_stubs,first,AndReturn,ReplayAll,vpnserv...  \n",
       "4  create_stubs,AndReturn,ReplayAll,vpnservices_g...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in processed file. A set of keywords for each document (source code function)\n",
    "unpickled_df = pd.read_pickle(\"data/py100.pkl\")\n",
    "func_size=len(unpickled_df)\n",
    "print(\"Total Number of Functions: {}\".format(func_size))\n",
    "unpickled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_func_name(terms):\n",
    "    \n",
    "    # remove punctuations except for \"_\"\n",
    "    ## reference: https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation.replace(\"_\", \"\")))\n",
    "    terms=regex.sub('', terms)\n",
    "    \n",
    "    #[TODO] camel case\n",
    "    \n",
    "    # [TODO] deal with __init__\n",
    "\n",
    "    # lowercase\n",
    "    terms=terms.lower()\n",
    "    \n",
    "    # snake case\n",
    "    return [term for term in terms.split(\"_\") if term!=\"\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each function, combine all the keywords into a set\n",
    "list_function_keywords=[]\n",
    "for idx in range(len(unpickled_df)):\n",
    "    keywords=[]\n",
    "    \n",
    "    func_name=parse_func_name(unpickled_df.iloc[idx][\"function_name\"].lower())\n",
    "    keywords+=func_name\n",
    "    \n",
    "    #[TODO] only alphabenumeric characters\n",
    "    #[TODO] camel case\n",
    "    #[TODO] snake case\n",
    "    \n",
    "    if unpickled_df.iloc[idx][\"docstring\"]:\n",
    "        docstring=unpickled_df.iloc[idx][\"docstring\"].lower().split()\n",
    "        keywords+=docstring\n",
    "    \n",
    "    if unpickled_df.iloc[idx][\"func_call\"]:\n",
    "        func_invoc=unpickled_df.iloc[idx][\"func_call\"].lower().split(\",\")\n",
    "        keywords+=func_invoc\n",
    "    \n",
    "    list_function_keywords.append(set(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1038"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_function_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "vocab_size=200\n",
    "window_size=5\n",
    "min_count=1\n",
    "\n",
    "\n",
    "# other parameters defined earlier\n",
    "# func_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We employ the continuous skip–gram model with a window size of 5, \n",
    "# i.e. all pairs of words within distance 5 are considered nearby words.\n",
    "\n",
    "#[TODO] tuning hyperparameters\n",
    "model = FastText(size=vocab_size, window=window_size, min_count=min_count)  # instantiate\n",
    "model.build_vocab(sentences=list_function_keywords)\n",
    "model.train(sentences=list_function_keywords, total_examples=len(list_function_keywords), epochs=10)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=2826, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a model trained via Gensim's fastText implementation\n",
    "model.save('saved_model_gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ft_vectors = model.wv\n",
    "# save vectors to file if you want to use them later\n",
    "trained_ft_vectors.save_word2vec_format('embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buttonpresssignal', 0.9999955296516418),\n",
       " ('buttonreleasesignal', 0.9999948740005493),\n",
       " ('menubutton', 0.9999943971633911),\n",
       " ('buttonpress', 0.999994158744812),\n",
       " ('buttonrelease', 0.9999939203262329),\n",
       " ('__menubutton', 0.9999935626983643),\n",
       " ('gadgetmenudefinition', 0.9999933838844299),\n",
       " ('widgetmenudefinition', 0.9999933242797852),\n",
       " ('__selectionchangedsignal', 0.9999933242797852),\n",
       " ('__addmenudefinition', 0.9999932646751404)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "trained_ft_vectors.most_similar(\"button\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Average over all the words;\n",
    "2. Average over the unique words in each document;\n",
    "3. [x] Weighted average of all unique words in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.56673110e-01,  5.04946597e-02, -6.21083099e-03,  7.66705275e-02,\n",
       "       -4.35648561e-01, -1.12217357e-02, -1.99079901e-01, -1.77537516e-01,\n",
       "       -1.90618366e-01, -1.70305610e-01,  2.27255642e-01,  9.98908356e-02,\n",
       "       -2.59666800e-01, -5.06281614e-01,  3.26456904e-01, -2.70832479e-01,\n",
       "        2.08521977e-01,  8.82439911e-02, -1.41606554e-01,  3.91939998e-01,\n",
       "        4.06734288e-01,  1.84914604e-01,  1.56045079e-01,  2.20082983e-01,\n",
       "        4.43840586e-02, -2.10403875e-01, -7.75189400e-01,  3.60961616e-01,\n",
       "        3.83777827e-01, -1.51074573e-01, -3.56423855e-02, -3.47285122e-02,\n",
       "        7.90149271e-02, -1.95222750e-01, -1.23928860e-01, -1.25188157e-01,\n",
       "       -2.44518846e-01,  1.82098448e-02,  2.50440568e-01,  2.17390880e-01,\n",
       "       -4.60534185e-01,  7.12547824e-02, -3.46408099e-01, -1.01377316e-01,\n",
       "        3.02482873e-01,  5.12988389e-01, -2.71733373e-01, -1.14690021e-01,\n",
       "        1.18469305e-01,  1.80004150e-01, -1.58470497e-01, -1.23429805e-01,\n",
       "        2.89993674e-01,  4.29745130e-02,  4.32484150e-02,  3.82406533e-01,\n",
       "        3.70398641e-01,  3.81750315e-01, -2.13712215e-01, -2.78004825e-01,\n",
       "        2.25257769e-04, -1.47096425e-01,  1.73277855e-01, -3.53924930e-01,\n",
       "        2.55506635e-01, -6.04994921e-03,  2.93171853e-01, -2.46514797e-01,\n",
       "       -1.71572432e-01, -2.86637604e-01, -1.21157579e-02, -4.07619812e-02,\n",
       "       -1.15780830e-01,  1.23230651e-01, -1.92165807e-01, -8.28727707e-02,\n",
       "        7.27444664e-02,  1.56160489e-01,  1.32716849e-01,  4.35871720e-01,\n",
       "       -2.40129098e-01,  1.13809511e-01, -1.96836174e-01, -2.64353245e-01,\n",
       "        6.05539456e-02, -2.03856096e-01,  8.27456713e-02, -2.89019555e-01,\n",
       "        3.36227298e-01,  3.66958618e-01, -9.81995612e-02,  9.83241871e-02,\n",
       "       -3.53194743e-01,  6.82035387e-02,  5.51223718e-02, -2.54287690e-01,\n",
       "       -6.44138530e-02, -1.30955800e-01, -3.22584689e-01,  3.33949119e-01,\n",
       "        2.05725571e-03, -2.05802977e-01, -1.25724182e-01,  1.30094633e-01,\n",
       "       -1.06672198e-01,  3.15335631e-01,  1.71667356e-02, -2.26455942e-01,\n",
       "        1.89047664e-01,  2.97529310e-01,  2.25866958e-02, -4.98101711e-01,\n",
       "       -2.41108641e-01,  4.27971371e-02,  2.16158956e-01, -9.40158293e-02,\n",
       "       -4.92928885e-02, -4.10600096e-01,  5.97459115e-02, -1.04270704e-01,\n",
       "       -5.22481315e-02, -2.41712108e-02, -2.63054252e-01, -1.63996756e-01,\n",
       "        2.65458137e-01,  1.69453010e-01, -2.29259402e-01,  1.75941601e-01,\n",
       "       -4.22919780e-01, -5.54240234e-02, -5.56030050e-02,  2.04374269e-01,\n",
       "        3.98062430e-02, -2.69392967e-01, -9.07530710e-02, -6.41990511e-04,\n",
       "        2.64817178e-02, -2.29180798e-01, -2.61669099e-01,  3.86959538e-02,\n",
       "        9.65965465e-02, -3.55255425e-01,  2.87967801e-01,  3.74262899e-01,\n",
       "       -6.61244243e-02,  8.97118226e-02,  1.17839336e-01,  5.95505871e-02,\n",
       "       -4.57421124e-01, -1.20868348e-01,  1.07097171e-01, -4.55758572e-02,\n",
       "        1.29658327e-01, -1.12021476e-01,  3.92160982e-01,  1.01033159e-01,\n",
       "       -6.20705113e-02,  5.53563237e-03,  7.26980790e-02, -1.89200416e-02,\n",
       "       -3.60582888e-01,  2.76478618e-01, -7.40608852e-03, -3.66292119e-01,\n",
       "        1.84664071e-01, -1.07957579e-01,  1.49579912e-01,  1.13715261e-01,\n",
       "       -1.87936112e-01,  1.82340294e-01, -2.79680807e-02,  4.73433994e-02,\n",
       "        4.38744247e-01,  8.93272683e-02,  1.85981169e-01, -3.61904740e-01,\n",
       "        3.10261339e-01, -2.95436054e-01,  2.13622704e-01, -6.06257059e-02,\n",
       "       -1.82837829e-01,  5.40209532e-01,  1.59767434e-01, -1.36475518e-01,\n",
       "        2.71962672e-01,  1.31702065e-01,  2.25039572e-01, -4.28991288e-01,\n",
       "        4.39220428e-01, -1.13804027e-01, -7.70573393e-02, -9.74364355e-02,\n",
       "        1.78654604e-02, -1.22677302e-02,  4.51779080e-04,  1.37998551e-01,\n",
       "       -2.70169765e-01, -3.73471469e-01,  1.71387762e-01, -7.21129104e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_ft_vectors[\"ping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings=np.zeros((func_size, vocab_size))\n",
    "for idx, doc in enumerate(list_function_keywords):\n",
    "    doc_vec_sum=np.zeros(vocab_size)\n",
    "    for term in doc:\n",
    "        doc_vec_sum+=trained_ft_vectors[term]\n",
    "    document_embeddings[idx]=doc_vec_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.78725350e-01,  8.05873852e-02, -9.74157732e-03,  1.20289113e-01,\n",
       "       -6.88482195e-01, -1.99429551e-02, -3.12518537e-01, -2.79037617e-01,\n",
       "       -3.01731780e-01, -2.65968807e-01,  3.59103099e-01,  1.56895529e-01,\n",
       "       -4.08433288e-01, -7.98545092e-01,  5.14272422e-01, -4.24994081e-01,\n",
       "        3.29313479e-01,  1.37901261e-01, -2.21873112e-01,  6.16696984e-01,\n",
       "        6.42413348e-01,  2.90032335e-01,  2.49334745e-01,  3.46673459e-01,\n",
       "        7.29946401e-02, -3.31593908e-01, -1.21915424e+00,  5.69301248e-01,\n",
       "        6.01602867e-01, -2.38119364e-01, -5.76344281e-02, -5.45627698e-02,\n",
       "        1.25188146e-01, -3.10112871e-01, -1.95762858e-01, -1.98347032e-01,\n",
       "       -3.84881064e-01,  2.96096532e-02,  3.90661761e-01,  3.46012399e-01,\n",
       "       -7.26857781e-01,  1.12263758e-01, -5.44764236e-01, -1.58682752e-01,\n",
       "        4.74034548e-01,  8.07780802e-01, -4.27958280e-01, -1.77960575e-01,\n",
       "        1.89581826e-01,  2.82897569e-01, -2.50179067e-01, -1.92018941e-01,\n",
       "        4.57203627e-01,  6.96069859e-02,  6.47026841e-02,  6.01725757e-01,\n",
       "        5.82485586e-01,  5.98796546e-01, -3.38288769e-01, -4.39758420e-01,\n",
       "        1.58473829e-03, -2.31969349e-01,  2.74158798e-01, -5.59909508e-01,\n",
       "        4.01029125e-01, -9.36709065e-03,  4.63532731e-01, -3.89223680e-01,\n",
       "       -2.70896487e-01, -4.50987712e-01, -1.79950260e-02, -6.25957232e-02,\n",
       "       -1.80885077e-01,  1.91799611e-01, -3.00242782e-01, -1.27882935e-01,\n",
       "        1.13435369e-01,  2.46967845e-01,  2.10288800e-01,  6.86696738e-01,\n",
       "       -3.78254503e-01,  1.79959230e-01, -3.08342524e-01, -4.15755361e-01,\n",
       "        9.20481458e-02, -3.21865149e-01,  1.32162217e-01, -4.54314977e-01,\n",
       "        5.30683309e-01,  5.74262887e-01, -1.52862288e-01,  1.54495656e-01,\n",
       "       -5.54668382e-01,  1.05925091e-01,  8.65858719e-02, -4.03261378e-01,\n",
       "       -1.02146417e-01, -2.04922602e-01, -5.06956458e-01,  5.26849180e-01,\n",
       "        3.51482548e-03, -3.24376769e-01, -1.98521271e-01,  2.05185644e-01,\n",
       "       -1.66078307e-01,  4.97715637e-01,  2.63215620e-02, -3.57686147e-01,\n",
       "        2.97249250e-01,  4.68881339e-01,  3.55129233e-02, -7.83975333e-01,\n",
       "       -3.79049197e-01,  6.85092509e-02,  3.40118296e-01, -1.47112522e-01,\n",
       "       -7.64296167e-02, -6.48673236e-01,  9.51526202e-02, -1.60742991e-01,\n",
       "       -8.31566155e-02, -3.90449185e-02, -4.13238883e-01, -2.58007146e-01,\n",
       "        4.18371528e-01,  2.68049218e-01, -3.61405507e-01,  2.76125491e-01,\n",
       "       -6.64514393e-01, -8.41467585e-02, -8.90283696e-02,  3.19075949e-01,\n",
       "        6.05175830e-02, -4.27037448e-01, -1.42945256e-01, -1.02921488e-03,\n",
       "        4.14938051e-02, -3.59652326e-01, -4.11941543e-01,  6.03989754e-02,\n",
       "        1.54098921e-01, -5.58350086e-01,  4.55286190e-01,  5.92116907e-01,\n",
       "       -1.02743104e-01,  1.43780787e-01,  1.88288122e-01,  9.10766087e-02,\n",
       "       -7.17079103e-01, -1.88929610e-01,  1.66918602e-01, -6.89414125e-02,\n",
       "        2.03695297e-01, -1.74184658e-01,  6.21361509e-01,  1.59130309e-01,\n",
       "       -9.92248878e-02,  1.10820006e-02,  1.17442477e-01, -2.78456137e-02,\n",
       "       -5.68344265e-01,  4.35612470e-01, -9.59396176e-03, -5.78124836e-01,\n",
       "        2.90517747e-01, -1.69785712e-01,  2.33971454e-01,  1.78592645e-01,\n",
       "       -2.95378417e-01,  2.85532497e-01, -4.48637661e-02,  7.55369794e-02,\n",
       "        6.88578874e-01,  1.42861485e-01,  2.94155851e-01, -5.70086047e-01,\n",
       "        4.89044294e-01, -4.64468017e-01,  3.34869452e-01, -9.39955376e-02,\n",
       "       -2.89569855e-01,  8.52555871e-01,  2.52481505e-01, -2.13652939e-01,\n",
       "        4.31429386e-01,  2.08555259e-01,  3.54194164e-01, -6.76226780e-01,\n",
       "        6.94633305e-01, -1.78698547e-01, -1.24170680e-01, -1.55515842e-01,\n",
       "        2.97097508e-02, -1.89901046e-02,  2.57724960e-03,  2.16523774e-01,\n",
       "       -4.24100548e-01, -5.90866342e-01,  2.67520256e-01, -1.17602471e-01])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1038 documents with 200 dimentions\n"
     ]
    }
   ],
   "source": [
    "print(\"{} documents with {} dimentions\".format(document_embeddings.shape[0], document_embeddings.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "The paper mentions 2 evaluation approach: 1 uses Github only, the other one uses both GitHub and StackOverflow. I'm guessing the former one is for tuning in the development stage; while the later is the final evaluation for the completed system (NCS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
