{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[150k Python Dataset](\"https://eth-sri.github.io/py150\") from SRILAB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuous vector embedding of each code fragment at method–level granularity as \"document\".[13] FastText, a variation of Word2Vec algorithm.\n",
    "- Extracting Information from Source Code\n",
    "    - simple tokenizer: extract all words from source code by removing non–alphanumeric tokens.=> indifferenciable\n",
    "    - parser-based approach: traverse through the parse tree for each method, and extract information from the following syntactic categories. (Java-like)\n",
    "        - method name\n",
    "        - method invocation\n",
    "        - Enums\n",
    "        - String literals\n",
    "        - comments\n",
    "        - <strike>variable name</strike>\n",
    "- Building Vector Representations\n",
    "    - <strike>simply average embeddings</strike>\n",
    "    - Weighted average of all unique words in a document=> normalized tf-idf\n",
    "- Retrieval\n",
    "    - average the vector representations of constituent words to create a document embedding for the query sentence\n",
    "    - a standard similarity search algorithm to find the document vectors with closest cosine distance. => FAISS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: natural language queries <br>\n",
    "Output: related code fragments retrieved directly from Github code corpus<br><br>\n",
    "\n",
    "Map the Input into the same vector space as the codebase, and then calculate the vector distance of them in order to get the relevant result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric and choosing parameters of the model\n",
    "\n",
    "- Metric: select subsets of words from the document as simulated queries and then see if it can retrive the document, and then evaluate by the percentage of the documents that are retrieve back at top1 and top10. \n",
    "    - random benchmark test\n",
    "    - TF-IDF benchmark test => better performance\n",
    "- Parameters:\n",
    "    - embedding dimention=> 500\n",
    "    - three ways of combining word embeddings to document embeddings=> the conclusion is tf-idf better\n",
    "    - vector representation=> BM25 is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Functions: 264\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>function_name</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>ping</td>\n",
       "      <td>Handle ping requests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>message</td>\n",
       "      <td>Proxy message from one user to another</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>presence</td>\n",
       "      <td>Presence information may be sent out from the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>roster</td>\n",
       "      <td>A roster is this account's list of contacts; i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>push</td>\n",
       "      <td>Push roster changes to all clients that have r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data_id function_name                                          docstring\n",
       "0       8          ping                               Handle ping requests\n",
       "1       8       message             Proxy message from one user to another\n",
       "2       8      presence  Presence information may be sent out from the ...\n",
       "3       8        roster  A roster is this account's list of contacts; i...\n",
       "4       8          push  Push roster changes to all clients that have r..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in processed file. A set of keywords for each document (source code function)\n",
    "unpickled_df = pd.read_pickle(\"./dummy.pkl\")\n",
    "func_size=len(unpickled_df)\n",
    "print(\"Total Number of Functions: {}\".format(func_size))\n",
    "unpickled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each function, combine all the keywords into a set\n",
    "list_function_keywords=[]\n",
    "for idx in range(len(unpickled_df)):\n",
    "    keywords=[]\n",
    "    keywords.append(unpickled_df.iloc[idx][\"function_name\"].lower())\n",
    "    \n",
    "    #[TODO] only alphabenumeric characters\n",
    "    #[TODO] camel case\n",
    "    #[TODO] snake case\n",
    "    #[TODO] Add function calls\n",
    "    docstring=unpickled_df.iloc[0][\"docstring\"].lower().split()\n",
    "        \n",
    "    \n",
    "    keywords+=docstring\n",
    "    list_function_keywords.append(set(keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "vocab_size=10\n",
    "window_size=5\n",
    "min_count=1\n",
    "\n",
    "\n",
    "# other parameters defined earlier\n",
    "# func_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We employ the continuous skip–gram model with a window size of 5, \n",
    "# i.e. all pairs of words within distance 5 are considered nearby words.\n",
    "\n",
    "#[TODO] tuning hyperparameters\n",
    "model = FastText(size=vocab_size, window=window_size, min_count=min_count)  # instantiate\n",
    "model.build_vocab(sentences=list_function_keywords)\n",
    "model.train(sentences=list_function_keywords, total_examples=len(list_function_keywords), epochs=10)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=212, size=10, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a model trained via Gensim's fastText implementation\n",
    "model.save('saved_model_gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ft_vectors = model.wv\n",
    "# save vectors to file if you want to use them later\n",
    "trained_ft_vectors.save_word2vec_format('embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('requests', 0.973355770111084),\n",
       " ('handle', 0.9630223512649536),\n",
       " ('help_ping', 0.8176469802856445),\n",
       " ('do_ping', 0.7732444405555725),\n",
       " ('find_registry', 0.6913332343101501),\n",
       " ('iterlists', 0.6796385049819946),\n",
       " ('close', 0.6605330109596252),\n",
       " ('test_divisibleby', 0.6550693511962891),\n",
       " ('add_header', 0.6526221036911011),\n",
       " ('do_list', 0.6403547525405884)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "trained_ft_vectors.most_similar(\"ping\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Average over all the words;\n",
    "2. Average over the unique words in each document;\n",
    "3. [x] Weighted average of all unique words in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03385561,  0.00126243, -0.03126138, -0.02985602, -0.06224034,\n",
       "       -0.02554143, -0.0580775 , -0.06109515, -0.11077709, -0.0576953 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_ft_vectors[\"ping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings=np.zeros((func_size, vocab_size))\n",
    "for idx, doc in enumerate(list_function_keywords):\n",
    "    doc_vec_sum=np.zeros(vocab_size)\n",
    "    for term in doc:\n",
    "        doc_vec_sum+=trained_ft_vectors[term]\n",
    "    document_embeddings[idx]=doc_vec_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14120819, -0.00069223, -0.11513096, -0.08152609, -0.14342172,\n",
       "       -0.08912421, -0.16949473, -0.18863287, -0.30467276, -0.17498178])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264 documents with 10 dimentions\n"
     ]
    }
   ],
   "source": [
    "print(\"{} documents with {} dimentions\".format(document_embeddings.shape[0], document_embeddings.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "The paper mentions 2 evaluation approach: 1 uses Github only, the other one uses both GitHub and StackOverflow. I'm guessing the former one is for tuning in the development stage; while the later is the final evaluation for the completed system (NCS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
