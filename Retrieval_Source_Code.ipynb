{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[150k Python Dataset](https://eth-sri.github.io/py150) from SRILAB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuous vector embedding of each code fragment at method–level granularity as \"document\".[13] FastText, a variation of Word2Vec algorithm.\n",
    "- Extracting Information from Source Code\n",
    "    - simple tokenizer: extract all words from source code by removing non–alphanumeric tokens.=> indifferenciable\n",
    "    - parser-based approach: traverse through the parse tree for each method, and extract information from the following syntactic categories. (Java-like)\n",
    "        - method name\n",
    "        - method invocation\n",
    "        - Enums\n",
    "        - String literals\n",
    "        - comments\n",
    "        - <strike>variable name</strike>\n",
    "- Building Vector Representations\n",
    "    - <strike>simply average embeddings</strike>\n",
    "    - Weighted average of all unique words in a document=> normalized tf-idf\n",
    "- Retrieval\n",
    "    - average the vector representations of constituent words to create a document embedding for the query sentence\n",
    "    - a standard similarity search algorithm to find the document vectors with closest cosine distance. => FAISS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: natural language queries <br>\n",
    "Output: related code fragments retrieved directly from Github code corpus<br><br>\n",
    "\n",
    "Map the Input into the same vector space as the codebase, and then calculate the vector distance of them in order to get the relevant result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric and choosing parameters of the model\n",
    "\n",
    "- Metric: select subsets of words from the document as simulated queries and then see if it can retrive the document, and then evaluate by the percentage of the documents that are retrieve back at top1 and top10. \n",
    "    - random benchmark test\n",
    "    - TF-IDF benchmark test => better performance\n",
    "- Parameters:\n",
    "    - embedding dimention=> 500\n",
    "    - three ways of combining word embeddings to document embeddings=> the conclusion is tf-idf better\n",
    "    - vector representation=> BM25 is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change parameters here\n",
    "function_file_path=\"pickle100.pkl\"\n",
    "wordembedding_file_path=\"data/embeddings.txt\"\n",
    "docembedding_file_path=\"data/document_embeddings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Functions in \"pickle100.pkl\": 1038\n"
     ]
    }
   ],
   "source": [
    "# load in processed file. A set of keywords for each document (source code function)\n",
    "def load_words_from_ast(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        function_list = pickle.load(f)\n",
    "    unpickled_df = pd.DataFrame(function_list, columns=['data_id', 'function_name', 'docstring', 'func_call'])\n",
    "\n",
    "    func_size=len(unpickled_df)\n",
    "    print(\"Total Number of Functions in \\\"{}\\\": {}\".format(file_path, func_size))\n",
    "    return unpickled_df\n",
    "\n",
    "unpickled_df=load_words_from_ast(function_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>function_name</th>\n",
       "      <th>docstring</th>\n",
       "      <th>func_call</th>\n",
       "      <th>keep_in_codebase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>__getattribute__</td>\n",
       "      <td></td>\n",
       "      <td>__getattribute__</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>__setattr__</td>\n",
       "      <td></td>\n",
       "      <td>ref,__setattr__</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>main</td>\n",
       "      <td></td>\n",
       "      <td>setup,closing,ZmqProxy,consume_in_thread,wait</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test_vpnservice_create</td>\n",
       "      <td></td>\n",
       "      <td>create_stubs,first,AndReturn,ReplayAll,vpnserv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>test_vpnservices_get</td>\n",
       "      <td></td>\n",
       "      <td>create_stubs,AndReturn,ReplayAll,vpnservices_g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_id           function_name docstring  \\\n",
       "0        1        __getattribute__             \n",
       "1        1             __setattr__             \n",
       "2        2                    main             \n",
       "3        3  test_vpnservice_create             \n",
       "4        3    test_vpnservices_get             \n",
       "\n",
       "                                           func_call  keep_in_codebase  \n",
       "0                                   __getattribute__                 0  \n",
       "1                                    ref,__setattr__                 1  \n",
       "2      setup,closing,ZmqProxy,consume_in_thread,wait                 1  \n",
       "3  create_stubs,first,AndReturn,ReplayAll,vpnserv...                 1  \n",
       "4  create_stubs,AndReturn,ReplayAll,vpnservices_g...                 1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# substract useless function from code base \n",
    "# 1 - keep, 0 - delete from code base\n",
    "unpickled_df['keep_in_codebase'] = np.where(((unpickled_df.function_name == unpickled_df.func_call)|\n",
    "                                             (unpickled_df['func_call'] == '')|\n",
    "                                            (unpickled_df['function_name'] == '__init__')\n",
    "                                            ), 0, 1)\n",
    "unpickled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_func_name(terms):\n",
    "    \"\"\"\n",
    "    Return a list of keywords from function name\n",
    "    \"\"\"\n",
    "    result=[]\n",
    "    #1. alphanumeric and \"-\", \"_\" only\n",
    "    terms=re.sub(\"[^\\w_-]\", \" \", terms)\n",
    "    \n",
    "    #2. snake case\n",
    "    result=[term for term in terms.split(\"_\") if term!=\"\"]\n",
    "    \n",
    "    #3. camel case\n",
    "    ## https://stackoverflow.com/questions/29916065/how-to-do-camelcase-split-in-python\n",
    "    def camel_case_split(identifier):\n",
    "        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "        return [m.group(0) for m in matches]\n",
    "    \n",
    "    result=[camel_case_split(t) for t in result]\n",
    "    \n",
    "    #flatten the result\n",
    "    #[TODO] check output\n",
    "    result=[s if isinstance(t, list) else t for t in result for s in t] \n",
    "    result=set(result)\n",
    "    # [TODO] deal with __init__\n",
    "\n",
    "    # lowercase\n",
    "    result=[t.lower() for t in result]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def parse_docstring(df):\n",
    "    #[TODO]remove punctuations except for \"_\"\n",
    "    ## reference: https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "    terms=df\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation.replace(\"_\", \"\")))\n",
    "    terms=regex.sub(' ', terms)\n",
    "    return terms\n",
    "    \n",
    "def parse_func_call(terms):\n",
    "    result=[]\n",
    "    terms_list=terms.split(\",\")\n",
    "    for term in terms_list:\n",
    "        result+=parse_func_name(term)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parsing examples\n",
    "##parse_docstring(\"test 123, real_test.\")\n",
    "#parse_func_name(\"UnpickledDfTest\")\n",
    "#parse_func_call(\"setup,closing,ZmqProxy,consume_in_thread,wait\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[TODO] wrap processing into pipeline\n",
    "#def parse_keywords(df):\n",
    "#    data = (\n",
    "#        data\n",
    "#        # Clean Data\n",
    "#        .pipe(parse_func_name)\n",
    "#        .pipe(fix_missing_desc)\n",
    "#        .pipe(change_lowercase_title)\n",
    "#        .pipe(change_lowercase_desc)\n",
    "#        \n",
    "#        # Transform data\n",
    "#        .pipe(select_columns, \n",
    "#              \"item_id\",\n",
    "#              \"title\",\n",
    "#              \"description\",\n",
    "#              \"deal_probability\"\n",
    "#             )\n",
    "#        .pipe(add_char_len_title)\n",
    "#        .pipe(add_char_len_desc)\n",
    "#        .pipe(add_tfidf_title)\n",
    "#        .pipe(add_tfidf_desc)\n",
    "#        #.pipe(add_keywords_desc)\n",
    "#    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each function, combine all the keywords into a set\n",
    "def parse_keywords(df):\n",
    "    list_function_keywords=[]\n",
    "    for idx in range(len(df)):\n",
    "        keywords=[]\n",
    "\n",
    "        func_name=parse_func_name(unpickled_df.iloc[idx][\"function_name\"])\n",
    "        keywords+=func_name\n",
    "        \n",
    "        # [TODO] exact possible function names (snakecase or camelcase)\n",
    "        if unpickled_df.iloc[idx][\"docstring\"]:\n",
    "            docstring=unpickled_df.iloc[idx][\"docstring\"].lower().split()\n",
    "            keywords+=docstring\n",
    "\n",
    "        if unpickled_df.iloc[idx][\"func_call\"]:\n",
    "            func_invoc=parse_func_call(unpickled_df.iloc[idx][\"func_call\"])\n",
    "            keywords+=func_invoc\n",
    "\n",
    "        list_function_keywords.append(set(keywords))\n",
    "    return list_function_keywords\n",
    "\n",
    "list_function_keywords=parse_keywords(unpickled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1038"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_function_keywords) #742490 for 100k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "#num_list_function_keywords=742490\n",
    "vocab_size=500\n",
    "window_size=5\n",
    "min_count=1\n",
    "\n",
    "\n",
    "# other parameters defined earlier\n",
    "func_size=len(unpickled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We employ the continuous skip–gram model with a window size of 5, \n",
    "# i.e. all pairs of words within distance 5 are considered nearby words.\n",
    "st=time()\n",
    "#[TODO] tuning hyperparameters\n",
    "model = FastText(size=vocab_size, window=window_size, min_count=min_count)  # instantiate\n",
    "model.build_vocab(sentences=list_function_keywords)\n",
    "model.train(sentences=list_function_keywords, total_examples=len(list_function_keywords), epochs=10)  # train\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=2187, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a model trained via Gensim's fastText implementation\n",
    "# 2019/03/05: the model might be too big. Saving word vector only.\n",
    "# model.save('saved_model_gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[TODO] normalize word mebedding\n",
    "trained_ft_vectors = model.wv\n",
    "# save vectors to file if you want to use them later\n",
    "trained_ft_vectors.save_word2vec_format(wordembedding_file_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 0.4404568672180176 s\n"
     ]
    }
   ],
   "source": [
    "# load wordembedding\n",
    "st=time()\n",
    "trained_ft_vectors = KeyedVectors.load_word2vec_format(wordembedding_file_path)\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('popitem', 0.9999849796295166),\n",
       " ('popitemlist', 0.9999837875366211),\n",
       " ('setitem', 0.9999821186065674),\n",
       " ('listvalues', 0.9999821186065674),\n",
       " ('version', 0.9999816417694092),\n",
       " ('versionadded::', 0.9999816417694092),\n",
       " ('formatted', 0.9999815225601196),\n",
       " ('serverprofiledescription:', 0.9999815225601196),\n",
       " ('authentication', 0.9999814629554749),\n",
       " ('additional', 0.9999814629554749)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "trained_ft_vectors.most_similar(\"pop\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Average over all the words;\n",
    "2. Average over the unique words in each document;\n",
    "3. [x] Weighted average of all unique words in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.48919451e-01,  1.51616216e-01,  1.49171427e-01, -1.45943258e-02,\n",
       "       -1.04194053e-01,  1.08259462e-01, -2.35674560e-01, -1.81718558e-01,\n",
       "       -2.63199985e-01,  2.06316218e-01, -1.80629060e-01,  2.53625661e-01,\n",
       "        2.92890608e-01, -5.10851681e-01,  2.78451264e-01,  7.92294323e-01,\n",
       "       -6.45732701e-01,  5.83183020e-02,  4.31636237e-02,  3.10843915e-01,\n",
       "        3.76761168e-01, -6.06286786e-02,  1.57849446e-01,  3.57999086e-01,\n",
       "       -4.04884905e-01,  2.36027073e-02, -7.58397877e-01,  3.18058223e-01,\n",
       "        1.97602212e-01,  9.15620625e-02,  2.65317738e-01, -1.63785771e-01,\n",
       "        1.16490103e-01, -1.65787116e-02,  1.83883920e-01, -4.76675510e-01,\n",
       "       -1.61798194e-03, -1.06274605e-01, -2.77244896e-01, -4.73161936e-02,\n",
       "        9.17183608e-02, -1.28844991e-01, -3.23222250e-01,  6.52275860e-01,\n",
       "        1.68597445e-01, -4.19973016e-01, -1.26212448e-01,  4.10499841e-01,\n",
       "       -1.86344922e-01,  3.78558725e-01, -2.56261110e-01, -2.56293386e-01,\n",
       "       -1.06969662e-01, -3.10269415e-01, -1.58135191e-01, -2.88118869e-01,\n",
       "        6.75739110e-01, -2.27895111e-01,  7.88528472e-02, -3.22444379e-01,\n",
       "       -3.72082025e-01,  8.85727108e-02, -1.86354816e-01, -3.75359446e-01,\n",
       "        6.22919947e-02,  5.15395701e-01, -2.46206716e-01,  6.55306354e-02,\n",
       "        2.09148526e-02, -1.67828321e-01, -1.41087279e-01,  2.02093899e-01,\n",
       "        8.36984366e-02, -6.21670261e-02,  4.68536198e-01,  2.66910344e-01,\n",
       "       -3.12732190e-01,  2.72427320e-01,  2.46070579e-01,  2.99429167e-02,\n",
       "       -5.53008467e-02,  2.83719361e-01, -2.07481056e-01,  1.24608884e-02,\n",
       "       -2.10289374e-01,  1.45269156e-01,  2.58835226e-01, -8.03591982e-02,\n",
       "       -2.23999977e-01,  3.41990650e-01, -1.29000813e-01,  1.00368954e-01,\n",
       "       -2.03698203e-01, -1.57277882e-01,  2.62297899e-01, -5.59404194e-01,\n",
       "        4.36528549e-02, -2.24180132e-01,  2.18634143e-01,  4.37338144e-01,\n",
       "       -1.82636783e-01, -1.05687760e-01, -1.19706631e-01,  1.42383635e-01,\n",
       "        5.22935152e-01, -9.37394649e-02,  3.94573301e-01, -2.16014296e-01,\n",
       "       -3.87457371e-01,  3.65137190e-01, -1.23510882e-01, -6.38557598e-02,\n",
       "       -7.62963519e-02, -3.01929444e-01,  2.61904120e-01,  5.70840612e-02,\n",
       "        2.11847484e-01, -2.60796458e-01,  2.29695171e-01,  2.44123459e-01,\n",
       "       -4.76368815e-01, -4.94934648e-01,  1.56523027e-02, -1.64462522e-01,\n",
       "       -7.59197176e-02,  2.33043328e-01,  2.12860286e-01, -3.65548581e-01,\n",
       "       -3.31581198e-02, -3.52568805e-01, -9.65669826e-02, -3.04219365e-01,\n",
       "       -8.42594385e-01, -2.64357358e-01, -7.63217881e-02,  7.23943301e-03,\n",
       "       -2.69975334e-01, -3.78818631e-01, -1.63860574e-01,  2.96898812e-01,\n",
       "        5.58174610e-01, -4.86369342e-01, -8.89395922e-02,  3.62277240e-01,\n",
       "       -1.08823903e-01, -1.66814223e-01, -2.08638653e-01, -1.10983914e-02,\n",
       "       -5.53666353e-01, -3.48149806e-01,  2.28100121e-01,  5.04117191e-01,\n",
       "        6.13113791e-02,  9.23344418e-02, -2.79427581e-02,  8.15468952e-02,\n",
       "       -3.03044859e-02,  2.23307565e-01,  1.05056621e-01,  4.26251441e-02,\n",
       "       -2.90342450e-01, -1.78206503e-01, -3.09572220e-01,  3.65185440e-02,\n",
       "       -2.30384439e-01,  6.48702383e-02, -8.04579183e-02, -7.48297647e-02,\n",
       "        4.62120622e-01,  1.97317339e-02,  3.24908823e-01,  1.35249272e-01,\n",
       "        3.43014628e-01,  1.10776126e-02, -3.82871181e-01,  9.32828486e-02,\n",
       "       -8.06083679e-02, -1.49718285e-01, -2.52970904e-01,  3.97469290e-02,\n",
       "       -5.93999634e-03,  2.97123104e-01,  2.78393656e-01,  4.69748154e-02,\n",
       "       -7.63463730e-04,  2.52745837e-01, -1.84111446e-01, -4.48645294e-01,\n",
       "        7.32035190e-02,  7.58139417e-02, -3.17862719e-01, -2.38422960e-01,\n",
       "        2.63057724e-02,  3.34445924e-01, -2.90159941e-01, -1.13415187e-02,\n",
       "       -1.92896560e-01,  3.32021788e-02, -2.22755913e-02,  5.50483942e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_ft_vectors[\"ping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=time()\n",
    "document_embeddings=np.zeros((func_size, vocab_size))\n",
    "for idx, doc in enumerate(list_function_keywords):\n",
    "    doc_vec_sum=np.zeros(vocab_size)\n",
    "    \n",
    "    for term in doc:\n",
    "        doc_vec_sum+=trained_ft_vectors[term]\n",
    "    \n",
    "    document_embeddings[idx]=doc_vec_sum/len(doc)\n",
    "    \n",
    "# save the whole document_embeddings\n",
    "np.savetxt(docembedding_file_path, document_embeddings, delimiter=\",\")\n",
    "print(\"Run time: {} s\".format(time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.21107592,  0.0904646 ,  0.08900643, -0.01055287, -0.0630949 ,\n",
       "        0.06544897, -0.14075872, -0.10933508, -0.15926448,  0.12434152,\n",
       "       -0.10959296,  0.15216427,  0.17697507, -0.31016997,  0.16759922,\n",
       "        0.47700399, -0.38974881,  0.03598445,  0.02537935,  0.1873029 ,\n",
       "        0.22739637, -0.03653921,  0.09591951,  0.2150961 , -0.24436121,\n",
       "        0.01391554, -0.45711553,  0.19186637,  0.11925419,  0.05474581,\n",
       "        0.16074289, -0.09895544,  0.07116915, -0.01077115,  0.11260857,\n",
       "       -0.28771931, -0.00127529, -0.06331161, -0.16753463, -0.02845665,\n",
       "        0.0566458 , -0.07940213, -0.19468884,  0.39402261,  0.10224196,\n",
       "       -0.25256968, -0.07602124,  0.24841116, -0.11205653,  0.22795437,\n",
       "       -0.15530488, -0.15433094, -0.06563385, -0.1853777 , -0.09565699,\n",
       "       -0.17379734,  0.4083873 , -0.13816503,  0.04775885, -0.19418085,\n",
       "       -0.22508293,  0.05323267, -0.1114646 , -0.22722116,  0.03826578,\n",
       "        0.31120721, -0.1502009 ,  0.03923115,  0.01323341, -0.10056369,\n",
       "       -0.08569429,  0.12161171,  0.0516354 , -0.03814477,  0.28297913,\n",
       "        0.16044457, -0.18884505,  0.16559511,  0.14808474,  0.01651116,\n",
       "       -0.03516172,  0.17077455, -0.12440515,  0.00729747, -0.12586088,\n",
       "        0.08749893,  0.15673436, -0.04709451, -0.1360216 ,  0.20512463,\n",
       "       -0.07803714,  0.06147497, -0.12389133, -0.09535465,  0.15914392,\n",
       "       -0.33645421,  0.026719  , -0.1358413 ,  0.13157262,  0.2645812 ,\n",
       "       -0.10945243, -0.06480306, -0.0722041 ,  0.08645355,  0.31570545,\n",
       "       -0.05731851,  0.23921266, -0.1303729 , -0.23321913,  0.21982157,\n",
       "       -0.07470541, -0.03704119, -0.045632  , -0.18241781,  0.15710823,\n",
       "        0.03494025,  0.12731035, -0.15715255,  0.1385157 ,  0.14814144,\n",
       "       -0.28659651, -0.29878283,  0.0088374 , -0.09982484, -0.0464958 ,\n",
       "        0.14136003,  0.12821457, -0.22085527, -0.01912162, -0.21263562,\n",
       "       -0.0581286 , -0.18156946, -0.50784206, -0.1586069 , -0.04573565,\n",
       "        0.00551125, -0.1625624 , -0.22810018, -0.09795357,  0.17955852,\n",
       "        0.33628434, -0.29415154, -0.05357452,  0.21791352, -0.06578223,\n",
       "       -0.09926271, -0.12606505, -0.00654812, -0.33327013, -0.21092428,\n",
       "        0.13798705,  0.3035976 ,  0.03523097,  0.05557713, -0.01778906,\n",
       "        0.04907383, -0.01826437,  0.13623653,  0.06268939,  0.02621416,\n",
       "       -0.17343293, -0.10663532, -0.1860476 ,  0.02163371, -0.13910066,\n",
       "        0.03937508, -0.04859076, -0.0439131 ,  0.27910009,  0.01097394,\n",
       "        0.19458443,  0.08139238,  0.20754193,  0.00612546, -0.23119481,\n",
       "        0.0562722 , -0.0475917 , -0.08979152, -0.15166846,  0.02516199,\n",
       "       -0.00287372,  0.17902744,  0.16886389,  0.0289274 , -0.00076661,\n",
       "        0.15123414, -0.1115872 , -0.26942796,  0.04395137,  0.04504897,\n",
       "       -0.19184241, -0.14321387,  0.01578572,  0.20067097, -0.17613342,\n",
       "       -0.00633999, -0.11551948,  0.01952469, -0.01386396,  0.3326934 ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1038 documents with 200 dimentions\n"
     ]
    }
   ],
   "source": [
    "print(\"{} documents with {} dimentions\".format(document_embeddings.shape[0], document_embeddings.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.7597290e-01,  1.1983531e-01,  1.1743154e-01, -1.1983320e-02,\n",
       "       -8.5660860e-02,  8.4647797e-02, -1.8512435e-01, -1.4348288e-01,\n",
       "       -2.0916961e-01,  1.6426457e-01, -1.4522627e-01,  2.0132948e-01,\n",
       "        2.3230293e-01, -4.0663308e-01,  2.1976487e-01,  6.2916785e-01,\n",
       "       -5.1324308e-01,  4.7161989e-02,  3.5263758e-02,  2.4826415e-01,\n",
       "        3.0012441e-01, -4.6042144e-02,  1.2703046e-01,  2.8364220e-01,\n",
       "       -3.2003599e-01,  1.8834826e-02, -6.0459703e-01,  2.5427550e-01,\n",
       "        1.5728919e-01,  7.2336636e-02,  2.1079786e-01, -1.2925409e-01,\n",
       "        9.4411172e-02, -1.2877812e-02,  1.4792378e-01, -3.7730339e-01,\n",
       "       -4.1531859e-04, -8.5618414e-02, -2.1905908e-01, -3.5493601e-02,\n",
       "        7.5856030e-02, -1.0389531e-01, -2.5891450e-01,  5.1983297e-01,\n",
       "        1.3484707e-01, -3.3604035e-01, -1.0199998e-01,  3.2755610e-01,\n",
       "       -1.4815851e-01,  3.0138874e-01, -2.0377013e-01, -2.0326264e-01,\n",
       "       -8.3405532e-02, -2.4334142e-01, -1.2641591e-01, -2.2769101e-01,\n",
       "        5.3916687e-01, -1.8147182e-01,  6.2284071e-02, -2.5677854e-01,\n",
       "       -2.9663092e-01,  6.9511645e-02, -1.4898729e-01, -3.0076998e-01,\n",
       "        4.8741613e-02,  4.1215530e-01, -1.9707732e-01,  5.0404780e-02,\n",
       "        2.0020217e-02, -1.2969744e-01, -1.1241479e-01,  1.6024373e-01,\n",
       "        6.6557065e-02, -4.9818512e-02,  3.7467685e-01,  2.1124069e-01,\n",
       "       -2.4966125e-01,  2.1520807e-01,  1.9353010e-01,  2.1999422e-02,\n",
       "       -4.4436123e-02,  2.2445093e-01, -1.6030182e-01,  1.0168701e-02,\n",
       "       -1.6470401e-01,  1.1468577e-01,  2.0897351e-01, -6.2857591e-02,\n",
       "       -1.7768085e-01,  2.7201048e-01, -1.0187640e-01,  8.1271060e-02,\n",
       "       -1.6541865e-01, -1.2486621e-01,  2.1027896e-01, -4.4604540e-01,\n",
       "        3.6312256e-02, -1.7980585e-01,  1.7274635e-01,  3.4638217e-01,\n",
       "       -1.4631496e-01, -8.6664096e-02, -9.6029498e-02,  1.1415399e-01,\n",
       "        4.1651168e-01, -7.3975526e-02,  3.1356186e-01, -1.6979311e-01,\n",
       "       -3.0733347e-01,  2.8842068e-01, -9.7522140e-02, -5.1980112e-02,\n",
       "       -5.9962969e-02, -2.3975234e-01,  2.0902944e-01,  4.5598846e-02,\n",
       "        1.6857557e-01, -2.0830955e-01,  1.8220970e-01,  1.9380207e-01,\n",
       "       -3.8027826e-01, -3.9264354e-01,  1.3207491e-02, -1.3306373e-01,\n",
       "       -5.8598001e-02,  1.8596551e-01,  1.7061912e-01, -2.9004258e-01,\n",
       "       -2.7949002e-02, -2.8274730e-01, -7.9306684e-02, -2.3974216e-01,\n",
       "       -6.7031401e-01, -2.0996104e-01, -6.0521264e-02,  6.6313734e-03,\n",
       "       -2.1305482e-01, -3.0112883e-01, -1.2823370e-01,  2.3536079e-01,\n",
       "        4.4418246e-01, -3.8648230e-01, -7.1931124e-02,  2.8692237e-01,\n",
       "       -8.9297190e-02, -1.3032272e-01, -1.6724403e-01, -1.0429944e-02,\n",
       "       -4.3949756e-01, -2.7666008e-01,  1.8087995e-01,  3.9704508e-01,\n",
       "        4.7170363e-02,  7.2282240e-02, -2.2557583e-02,  6.5333694e-02,\n",
       "       -2.5652425e-02,  1.7613329e-01,  8.4353678e-02,  3.6064718e-02,\n",
       "       -2.3158966e-01, -1.4071612e-01, -2.4424049e-01,  2.8277371e-02,\n",
       "       -1.8450384e-01,  5.3532805e-02, -6.4240180e-02, -5.6947071e-02,\n",
       "        3.7155527e-01,  1.3699131e-02,  2.5685281e-01,  1.0945081e-01,\n",
       "        2.7269253e-01,  5.0026267e-03, -3.0439806e-01,  7.4449264e-02,\n",
       "       -6.4434193e-02, -1.2080171e-01, -1.9981487e-01,  3.1563379e-02,\n",
       "       -4.1499543e-03,  2.3537816e-01,  2.2156124e-01,  3.4939002e-02,\n",
       "       -1.1757737e-03,  1.9945665e-01, -1.4883439e-01, -3.5805842e-01,\n",
       "        5.7908032e-02,  6.0331214e-02, -2.5172248e-01, -1.8946148e-01,\n",
       "        2.0194700e-02,  2.6391003e-01, -2.3098801e-01, -9.6179117e-03,\n",
       "       -1.5125573e-01,  2.7987659e-02, -1.6323421e-02,  4.3774897e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_ft_vectors[\"pop\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "The paper mentions 2 evaluation approach: 1 uses Github only, the other one uses both GitHub and StackOverflow. I'm guessing the former one is for tuning in the development stage; while the later is the final evaluation for the completed system (NCS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
